import * as assert from "assert";
import {
  TokenGraphemeSplitter,
  UNKNOWN,
} from "../../core/TokenGraphemeSplitter";
import { Graph, TokenHatSplittingMode } from "../../typings/Types";
import makeGraph, { FactoryMap } from "../../util/makeGraph";
import { FakeIDE } from "./fakes/ide/FakeIDE";

/**
 * Compact representation of a grapheme to make the tests easier to read.
 * Expected to be of the form
 * [text, tokenStartOffset, tokenEndOffset]
 */
type CompactGrapheme = [string, number, number];

/**
 * A compact representation of a test case. Expected to be of the form
 * [input, expectedOutput]
 */
type TestCase = [string, CompactGrapheme[]];

interface SplittingModeTestCases {
  tokenHatSplittingMode: Partial<TokenHatSplittingMode>;
  extraTestCases: TestCase[];
}

const commonTestCases: TestCase[] = [
  [
    "hi",
    [
      ["h", 0, 1],
      ["i", 1, 2],
    ],
  ],
  ["_", [["_", 0, 1]]],
  ["ðŸ˜„", [[UNKNOWN, 0, 2]]],
];

const tests: SplittingModeTestCases[] = [
  {
    tokenHatSplittingMode: {
      preserveAccents: true,
    },
    extraTestCases: [
      [
        "\u00F1", // Ã± as single codepoint
        [["\u00F1", 0, 1]],
      ],
      [
        "\u006E\u0303", // Ã± using combining mark
        [["\u00F1", 0, 2]],
      ],
      [
        "\u00D1", // Ã‘ as single codepoint
        [["\u00F1", 0, 1]],
      ],
      [
        "\u004E\u0303", // Ã‘ using combining mark
        [["\u00F1", 0, 2]],
      ],
      [
        "Hi",
        [
          ["h", 0, 1],
          ["i", 1, 2],
        ],
      ],
      ["ê", [["ê", 0, 1]]],
      ["Ã¸", [["Ã¸", 0, 1]]],
      ["êŽ", [["ê", 0, 1]]],
      ["Ã˜", [["Ã¸", 0, 1]]],
    ],
  },
  {
    tokenHatSplittingMode: {
      preserveCase: true,
      preserveAccents: true,
    },
    extraTestCases: [
      [
        "\u00F1", // Ã± as single codepoint
        [["\u00F1", 0, 1]],
      ],
      [
        "\u006E\u0303", // Ã± using combining mark
        [["\u00F1", 0, 2]],
      ],
      [
        "\u00D1", // Ã‘ as single codepoint
        [["\u00D1", 0, 1]],
      ],
      [
        "\u004E\u0303", // Ã‘ using combining mark
        [["\u00D1", 0, 2]],
      ],
      [
        "Hi",
        [
          ["H", 0, 1],
          ["i", 1, 2],
        ],
      ],
      ["ê", [["ê", 0, 1]]],
      ["Ã¸", [["Ã¸", 0, 1]]],
      ["êŽ", [["êŽ", 0, 1]]],
      ["Ã˜", [["Ã˜", 0, 1]]],
    ],
  },
  {
    tokenHatSplittingMode: {
      preserveCase: true,
    },
    extraTestCases: [
      [
        "\u00F1", // Ã± as single codepoint
        [["n", 0, 1]],
      ],
      [
        "\u006E\u0303", // Ã± using combining mark
        [["n", 0, 2]],
      ],
      [
        "\u00D1", // Ã‘ as single codepoint
        [["N", 0, 1]],
      ],
      [
        "\u004E\u0303", // Ã‘ using combining mark
        [["N", 0, 2]],
      ],
      ["ê", [[UNKNOWN, 0, 1]]],
      ["Ã¸", [["o", 0, 1]]],
      ["êŽ", [[UNKNOWN, 0, 1]]],
      ["Ã˜", [["O", 0, 1]]],
    ],
  },
  {
    tokenHatSplittingMode: {},
    extraTestCases: [
      [
        "\u00F1", // Ã± as single codepoint
        [["n", 0, 1]],
      ],
      [
        "\u006E\u0303", // Ã± using combining mark
        [["n", 0, 2]],
      ],
      [
        "\u00D1", // Ã‘ as single codepoint
        [["n", 0, 1]],
      ],
      [
        "\u004E\u0303", // Ã‘ using combining mark
        [["n", 0, 2]],
      ],
      ["ê", [[UNKNOWN, 0, 1]]],
      ["Ã¸", [["o", 0, 1]]],
      ["êŽ", [[UNKNOWN, 0, 1]]],
      ["Ã˜", [["o", 0, 1]]],
    ],
  },
  {
    tokenHatSplittingMode: {
      accentsToPreserve: ["\u00e4", "\u00e5"], // Ã¤Ã¥, NFC-normalised
    },
    extraTestCases: [
      [
        "\u00F1", // Ã± as single codepoint
        [["n", 0, 1]],
      ],
      [
        "\u006E\u0303", // Ã± using combining mark
        [["n", 0, 2]],
      ],
      [
        "\u00D1", // Ã‘ as single codepoint
        [["n", 0, 1]],
      ],
      [
        "\u004E\u0303", // Ã‘ using combining mark
        [["n", 0, 2]],
      ],
      [
        "\u00e4\u00e5", // Ã¤Ã¥, NFC-normalised
        [
          ["\u00e4", 0, 1], // Ã¤, NFC-normalised
          ["\u00e5", 1, 2], // Ã¥, NFC-normalised
        ],
      ],
      [
        "\u0061\u0308\u0061\u030a", // aÌˆaÌŠ, NFD-normalised
        [
          ["\u00e4", 0, 2], // Ã¤, NFC-normalised
          ["\u00e5", 2, 4], // Ã¥, NFC-normalised
        ],
      ],
      [
        "\u00c4\u00c5", // Ã„Ã…, NFC-normalised
        [
          ["\u00e4", 0, 1], // Ã¤, NFC-normalised
          ["\u00e5", 1, 2], // Ã¥, NFC-normalised
        ],
      ],
      [
        "\u0041\u0308\u0041\u030a", // AÌˆAÌŠ, NFD-normalised
        [
          ["\u00e4", 0, 2], // Ã¤, NFC-normalised
          ["\u00e5", 2, 4], // Ã¥, NFC-normalised
        ],
      ],
    ],
  },
  {
    tokenHatSplittingMode: {
      accentsToPreserve: ["\u0061\u0308", "\u0061\u030a"], // aÌˆaÌŠ, NFD-normalised
    },
    extraTestCases: [
      [
        "\u00e4\u00e5", // Ã¤Ã¥, NFC-normalised
        [
          ["\u00e4", 0, 1],
          ["\u00e5", 1, 2],
        ],
      ],
      [
        "\u0061\u0308\u0061\u030a", // aÌˆaÌŠ, NFD-normalised
        [
          ["\u00e4", 0, 2],
          ["\u00e5", 2, 4],
        ],
      ],
    ],
  },
  {
    tokenHatSplittingMode: {
      preserveCase: true,
      accentsToPreserve: ["\u00e4", "\u00e5"], // Ã¤Ã¥, NFC-normalised
    },
    extraTestCases: [
      [
        "\u00e4\u00e5", // Ã¤Ã¥, NFC-normalised
        [
          ["\u00e4", 0, 1],
          ["\u00e5", 1, 2],
        ],
      ],
      [
        "\u00c4\u00c5", // Ã„Ã…, NFC-normalised
        [
          ["\u00c4", 0, 1], // Ã„, NFC-normalised
          ["\u00c5", 1, 2], // Ã…, NFC-normalised
        ],
      ],
    ],
  },
  {
    tokenHatSplittingMode: {
      accentsToPreserve: ["\u00c4", "\u00c5"], // Ã„Ã…, NFC-normalised
    },
    extraTestCases: [
      [
        "\u00e4\u00e5", // Ã¤Ã¥, NFC-normalised
        [
          ["\u00e4", 0, 1], // Ã¤, NFC-normalised
          ["\u00e5", 1, 2], // Ã¥, NFC-normalised
        ],
      ],
      [
        "\u00c4\u00c5", // Ã„Ã…, NFC-normalised
        [
          ["\u00e4", 0, 1], // Ã¤, NFC-normalised
          ["\u00e5", 1, 2], // Ã¥, NFC-normalised
        ],
      ],
    ],
  },
  {
    tokenHatSplittingMode: {
      symbolsToPreserve: ["ðŸ™ƒ"],
    },
    extraTestCases: [
      [
        "\u00F1", // Ã± as single codepoint
        [["n", 0, 1]],
      ],
      [
        "\u006E\u0303", // Ã± using combining mark
        [["n", 0, 2]],
      ],
      [
        "\u00D1", // Ã‘ as single codepoint
        [["n", 0, 1]],
      ],
      [
        "\u004E\u0303", // Ã‘ using combining mark
        [["n", 0, 2]],
      ],
      ["ðŸ™ƒ", [["ðŸ™ƒ", 0, 2]]],
    ],
  },
];

const graph = makeGraph({
  tokenGraphemeSplitter: (graph: Graph) => new TokenGraphemeSplitter(graph),
  ide: (graph: Graph) => new FakeIDE(graph),
} as unknown as FactoryMap<Graph>);

const tokenHatSplittingDefaults: TokenHatSplittingMode = {
  preserveCase: false,
  preserveAccents: false,
  accentsToPreserve: [],
  symbolsToPreserve: [],
};

graph.ide.configuration.mockConfiguration(
  "tokenHatSplittingMode",
  tokenHatSplittingDefaults
);

const tokenGraphemeSplitter = graph.tokenGraphemeSplitter;

tests.forEach(({ tokenHatSplittingMode, extraTestCases }) => {
  suite(`getTokenGraphemes(${JSON.stringify(tokenHatSplittingMode)})`, () => {
    graph.ide.configuration.mockConfiguration("tokenHatSplittingMode", {
      ...tokenHatSplittingDefaults,
      ...tokenHatSplittingMode,
    });

    const testCases = [...commonTestCases, ...extraTestCases];

    testCases.forEach(([input, compactExpectedOutput]) => {
      const expectedOutput = compactExpectedOutput.map(
        ([text, tokenStartOffset, tokenEndOffset]) => ({
          text,
          tokenStartOffset,
          tokenEndOffset,
        })
      );

      const actualOutput = tokenGraphemeSplitter.getTokenGraphemes(input);

      test(input, () => {
        assert.deepStrictEqual(actualOutput, expectedOutput);
      });
    });
  });
});
