import type { TokenHatSplittingMode } from "@cursorless/common";
import * as assert from "assert";
import { unitTestSetup } from "../test/unitTestSetup";
import { TokenGraphemeSplitter, UNKNOWN } from "./tokenGraphemeSplitter";

/**
 * Compact representation of a grapheme to make the tests easier to read.
 * Expected to be of the form
 * [text, tokenStartOffset, tokenEndOffset]
 */
type CompactGrapheme = [string, number, number];

/**
 * A compact representation of a test case. Expected to be of the form
 * [input, expectedOutput]
 */
type TestCase = [string, CompactGrapheme[]];

interface SplittingModeTestCases {
  tokenHatSplittingMode: Partial<TokenHatSplittingMode>;
  extraTestCases: TestCase[];
}

const commonTestCases: TestCase[] = [
  [
    "hi",
    [
      ["h", 0, 1],
      ["i", 1, 2],
    ],
  ],
  ["_", [["_", 0, 1]]],
  ["ðŸ˜„", [[UNKNOWN, 0, 2]]],
];

const tests: SplittingModeTestCases[] = [
  {
    tokenHatSplittingMode: {},
    extraTestCases: [
      [
        "\u00F1", // Ã± as single codepoint
        [["n", 0, 1]],
      ],
      [
        "\u006E\u0303", // Ã± using combining mark
        [["n", 0, 2]],
      ],
      [
        "\u00D1", // Ã‘ as single codepoint
        [["n", 0, 1]],
      ],
      [
        "\u004E\u0303", // Ã‘ using combining mark
        [["n", 0, 2]],
      ],
      ["ê", [[UNKNOWN, 0, 1]]],
      ["Ã¸", [["o", 0, 1]]],
      ["Ã¦", [[UNKNOWN, 0, 1]]],
      ["êŽ", [[UNKNOWN, 0, 1]]],
      ["Ã˜", [["o", 0, 1]]],
      ["Ã†", [[UNKNOWN, 0, 1]]],
      ["Î£", [[UNKNOWN, 0, 1]]],
      ["Ïƒ", [[UNKNOWN, 0, 1]]],
    ],
  },
  {
    tokenHatSplittingMode: {
      preserveCase: true,
    },
    extraTestCases: [
      [
        "\u00F1", // Ã± as single codepoint
        [["n", 0, 1]],
      ],
      [
        "\u006E\u0303", // Ã± using combining mark
        [["n", 0, 2]],
      ],
      [
        "\u00D1", // Ã‘ as single codepoint
        [["N", 0, 1]],
      ],
      [
        "\u004E\u0303", // Ã‘ using combining mark
        [["N", 0, 2]],
      ],
      ["ê", [[UNKNOWN, 0, 1]]],
      ["Ã¸", [["o", 0, 1]]],
      ["Ã¦", [[UNKNOWN, 0, 1]]],
      ["êŽ", [[UNKNOWN, 0, 1]]],
      ["Ã˜", [["O", 0, 1]]],
      ["Ã†", [[UNKNOWN, 0, 1]]],
    ],
  },
  {
    tokenHatSplittingMode: {
      lettersToPreserve: [
        "\u00e4", // Ã¤, NFC-normalised
        "\u00e5", // Ã¥, NFC-normalised
        "ê",
        "Ã¸",
        "Ã¦",
      ],
    },
    extraTestCases: [
      [
        "\u00F1", // Ã± as single codepoint
        [["n", 0, 1]],
      ],
      [
        "\u006E\u0303", // Ã± using combining mark
        [["n", 0, 2]],
      ],
      [
        "\u00D1", // Ã‘ as single codepoint
        [["n", 0, 1]],
      ],
      [
        "\u004E\u0303", // Ã‘ using combining mark
        [["n", 0, 2]],
      ],
      [
        "\u00e4\u00e5", // Ã¤Ã¥, NFC-normalised
        [
          ["\u00e4", 0, 1], // Ã¤, NFC-normalised
          ["\u00e5", 1, 2], // Ã¥, NFC-normalised
        ],
      ],
      [
        "\u0061\u0308\u0061\u030a", // aÌˆaÌŠ, NFD-normalised
        [
          ["\u00e4", 0, 2], // Ã¤, NFC-normalised
          ["\u00e5", 2, 4], // Ã¥, NFC-normalised
        ],
      ],
      [
        "\u00c4\u00c5", // Ã„Ã…, NFC-normalised
        [
          ["\u00e4", 0, 1], // Ã¤, NFC-normalised
          ["\u00e5", 1, 2], // Ã¥, NFC-normalised
        ],
      ],
      [
        "\u0041\u0308\u0041\u030a", // AÌˆAÌŠ, NFD-normalised
        [
          ["\u00e4", 0, 2], // Ã¤, NFC-normalised
          ["\u00e5", 2, 4], // Ã¥, NFC-normalised
        ],
      ],
      ["ê", [["ê", 0, 1]]],
      ["Ã¸", [["Ã¸", 0, 1]]],
      ["Ã¦", [["Ã¦", 0, 1]]],
      ["êŽ", [["ê", 0, 1]]],
      ["Ã˜", [["Ã¸", 0, 1]]],
      ["Ã†", [["Ã¦", 0, 1]]],
    ],
  },
  {
    tokenHatSplittingMode: {
      lettersToPreserve: [
        "\u0061\u0308", // aÌˆ, NFD-normalised
        "\u0061\u030a", // aÌŠ, NFD-normalised
      ],
    },
    extraTestCases: [
      [
        "\u00e4\u00e5", // Ã¤Ã¥, NFC-normalised
        [
          ["\u00e4", 0, 1],
          ["\u00e5", 1, 2],
        ],
      ],
      [
        "\u0061\u0308\u0061\u030a", // aÌˆaÌŠ, NFD-normalised
        [
          ["\u00e4", 0, 2],
          ["\u00e5", 2, 4],
        ],
      ],
    ],
  },
  {
    tokenHatSplittingMode: {
      preserveCase: true,
      lettersToPreserve: [
        "\u00e4", // Ã¤, NFC-normalised
        "\u00e5", // Ã¥, NFC-normalised
        "ê",
        "Ã¸",
        "Ã¦",
      ],
    },
    extraTestCases: [
      [
        "\u00e4\u00e5", // Ã¤Ã¥, NFC-normalised
        [
          ["\u00e4", 0, 1],
          ["\u00e5", 1, 2],
        ],
      ],
      [
        "\u00c4\u00c5", // Ã„Ã…, NFC-normalised
        [
          ["\u00c4", 0, 1], // Ã„, NFC-normalised
          ["\u00c5", 1, 2], // Ã…, NFC-normalised
        ],
      ],
      ["ê", [["ê", 0, 1]]],
      ["Ã¸", [["Ã¸", 0, 1]]],
      ["Ã¦", [["Ã¦", 0, 1]]],
      ["êŽ", [["êŽ", 0, 1]]],
      ["Ã˜", [["Ã˜", 0, 1]]],
      ["Ã†", [["Ã†", 0, 1]]],
    ],
  },
  {
    tokenHatSplittingMode: {
      lettersToPreserve: [
        "\u00c4", // Ã„, NFC-normalised
        "\u00c5", // Ã…, NFC-normalised
        "êŽ",
        "Ã˜",
        "Ã†",
      ],
    },
    extraTestCases: [
      [
        "\u00e4\u00e5", // Ã¤Ã¥, NFC-normalised
        [
          ["\u00e4", 0, 1], // Ã¤, NFC-normalised
          ["\u00e5", 1, 2], // Ã¥, NFC-normalised
        ],
      ],
      [
        "\u00c4\u00c5", // Ã„Ã…, NFC-normalised
        [
          ["\u00e4", 0, 1], // Ã¤, NFC-normalised
          ["\u00e5", 1, 2], // Ã¥, NFC-normalised
        ],
      ],
      ["ê", [["ê", 0, 1]]],
      ["Ã¸", [["Ã¸", 0, 1]]],
      ["Ã¦", [["Ã¦", 0, 1]]],
      ["êŽ", [["ê", 0, 1]]],
      ["Ã˜", [["Ã¸", 0, 1]]],
      ["Ã†", [["Ã¦", 0, 1]]],
    ],
  },
  {
    tokenHatSplittingMode: {
      symbolsToPreserve: ["ðŸ™ƒ", "Î£", "Ïƒ"],
    },
    extraTestCases: [
      [
        "\u00F1", // Ã± as single codepoint
        [["n", 0, 1]],
      ],
      [
        "\u006E\u0303", // Ã± using combining mark
        [["n", 0, 2]],
      ],
      [
        "\u00D1", // Ã‘ as single codepoint
        [["n", 0, 1]],
      ],
      [
        "\u004E\u0303", // Ã‘ using combining mark
        [["n", 0, 2]],
      ],
      ["Î£", [["Î£", 0, 1]]],
      ["Ïƒ", [["Ïƒ", 0, 1]]],
    ],
  },
];

const tokenHatSplittingDefaults: TokenHatSplittingMode = {
  preserveCase: false,
  lettersToPreserve: [],
  symbolsToPreserve: [],
};

tests.forEach(({ tokenHatSplittingMode, extraTestCases }) => {
  suite(`getTokenGraphemes(${JSON.stringify(tokenHatSplittingMode)})`, () => {
    unitTestSetup(({ configuration }) => {
      configuration.mockConfiguration("tokenHatSplittingMode", {
        ...tokenHatSplittingDefaults,
        ...tokenHatSplittingMode,
      });
    });

    const testCases = [...commonTestCases, ...extraTestCases];

    testCases.forEach(([input, compactExpectedOutput]) => {
      const expectedOutput = compactExpectedOutput.map(
        ([text, tokenStartOffset, tokenEndOffset]) => ({
          text,
          tokenStartOffset,
          tokenEndOffset,
        }),
      );

      const displayOutput = expectedOutput.map(({ text }) => text).join(", ");

      test(`${input} -> ${displayOutput}`, () => {
        const actualOutput = new TokenGraphemeSplitter().getTokenGraphemes(
          input,
        );
        assert.deepStrictEqual(actualOutput, expectedOutput);
      });
    });
  });
});
